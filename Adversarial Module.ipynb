{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow_decision_forests as tfdf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import classification_report, roc_auc_score,accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "import time\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weight</th>\n",
       "      <th>Body mass index</th>\n",
       "      <th>Systolic</th>\n",
       "      <th>Diastolic</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>Glycohemoglobin</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>High-density lipoprotein (HDL)</th>\n",
       "      <th>...</th>\n",
       "      <th>Basophils</th>\n",
       "      <th>Red blood cells</th>\n",
       "      <th>Hemoglobin</th>\n",
       "      <th>Red blood cell width</th>\n",
       "      <th>Platelet count</th>\n",
       "      <th>Mean volume of platelets</th>\n",
       "      <th>Coronary heart disease</th>\n",
       "      <th>Blood related diabetes</th>\n",
       "      <th>Moderate-work</th>\n",
       "      <th>Vigorous-work</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92.5</td>\n",
       "      <td>29.10</td>\n",
       "      <td>122.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>597.0</td>\n",
       "      <td>No</td>\n",
       "      <td>5.5</td>\n",
       "      <td>7.21</td>\n",
       "      <td>1.08</td>\n",
       "      <td>...</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>5.13</td>\n",
       "      <td>14.5</td>\n",
       "      <td>13.1</td>\n",
       "      <td>209.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>78.0</td>\n",
       "      <td>29.39</td>\n",
       "      <td>130.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>712.0</td>\n",
       "      <td>No</td>\n",
       "      <td>5.8</td>\n",
       "      <td>6.34</td>\n",
       "      <td>2.73</td>\n",
       "      <td>...</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>4.60</td>\n",
       "      <td>13.4</td>\n",
       "      <td>14.3</td>\n",
       "      <td>244.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>111.8</td>\n",
       "      <td>30.94</td>\n",
       "      <td>152.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>518.0</td>\n",
       "      <td>No</td>\n",
       "      <td>5.5</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1.31</td>\n",
       "      <td>...</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>5.00</td>\n",
       "      <td>15.4</td>\n",
       "      <td>13.7</td>\n",
       "      <td>167.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>No</td>\n",
       "      <td>Don't know</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Don't know</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>75.5</td>\n",
       "      <td>27.33</td>\n",
       "      <td>142.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>973.0</td>\n",
       "      <td>No</td>\n",
       "      <td>5.8</td>\n",
       "      <td>4.50</td>\n",
       "      <td>1.04</td>\n",
       "      <td>...</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>5.32</td>\n",
       "      <td>16.6</td>\n",
       "      <td>12.4</td>\n",
       "      <td>160.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>81.6</td>\n",
       "      <td>26.68</td>\n",
       "      <td>106.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>459.0</td>\n",
       "      <td>No</td>\n",
       "      <td>4.6</td>\n",
       "      <td>5.15</td>\n",
       "      <td>1.49</td>\n",
       "      <td>...</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>4.14</td>\n",
       "      <td>13.3</td>\n",
       "      <td>11.9</td>\n",
       "      <td>255.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Weight  Body mass index  Systolic  Diastolic  Gender    Age Diabetes  \\\n",
       "4     92.5            29.10     122.0       82.0    Male  597.0       No   \n",
       "6     78.0            29.39     130.0       78.0  Female  712.0       No   \n",
       "9    111.8            30.94     152.0       98.0    Male  518.0       No   \n",
       "13    75.5            27.33     142.0       56.0    Male  973.0       No   \n",
       "14    81.6            26.68     106.0       68.0  Female  459.0       No   \n",
       "\n",
       "    Glycohemoglobin  Cholesterol  High-density lipoprotein (HDL)  ...  \\\n",
       "4               5.5         7.21                            1.08  ...   \n",
       "6               5.8         6.34                            2.73  ...   \n",
       "9               5.5         3.62                            1.31  ...   \n",
       "13              5.8         4.50                            1.04  ...   \n",
       "14              4.6         5.15                            1.49  ...   \n",
       "\n",
       "       Basophils  Red blood cells  Hemoglobin  Red blood cell width  \\\n",
       "4   5.397605e-79             5.13        14.5                  13.1   \n",
       "6   5.397605e-79             4.60        13.4                  14.3   \n",
       "9   5.397605e-79             5.00        15.4                  13.7   \n",
       "13  5.397605e-79             5.32        16.6                  12.4   \n",
       "14  5.397605e-79             4.14        13.3                  11.9   \n",
       "\n",
       "    Platelet count  Mean volume of platelets  Coronary heart disease  \\\n",
       "4            209.0                      10.4                      No   \n",
       "6            244.0                       8.2                      No   \n",
       "9            167.0                       9.4                      No   \n",
       "13           160.0                       9.0                      No   \n",
       "14           255.0                       7.7                      No   \n",
       "\n",
       "    Blood related diabetes  Moderate-work  Vigorous-work  \n",
       "4                       No           17.0            Yes  \n",
       "6                      Yes            3.0             No  \n",
       "9               Don't know           13.0     Don't know  \n",
       "13                      No            9.0            Yes  \n",
       "14                      No           13.0            Yes  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet('data/nhanes_data_processed.parquet')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_load_data(data = pd.DataFrame, target = str):\n",
    "    #The input should only be a Pandas DataFrame \n",
    " \n",
    "    #This creates split datasets for training, testing, and validation\n",
    "    #Additionally it prepares the input data sets for model fitting and predicting\n",
    "    X = data.drop(target, axis = 1)\n",
    "    y = data[target]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    X_val =  scaler.transform(X_val)\n",
    "\n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Model - Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Number of rows and columns\n",
    "num_rows = 1000\n",
    "num_columns = 41\n",
    "\n",
    "# Generate random float data for 39 columns\n",
    "float_data = np.random.rand(num_rows, num_columns - 2)\n",
    "\n",
    "# Generate two binary columns\n",
    "binary_data = np.random.randint(0, 2, size=(num_rows, 2))\n",
    "\n",
    "# Combine into a single dataset\n",
    "data = np.hstack((float_data, binary_data))\n",
    "\n",
    "# Create column names\n",
    "column_names = [f\"feature_{i}\" for i in range(1, num_columns - 1)]\n",
    "column_names.append(\"Gender\")\n",
    "column_names.append(\"CHD\")\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CHD'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = df.columns[:-1]  # All columns except the last two (includes sensitive feature)\n",
    "sensitive_column = df.columns[-2]  # The third last column as the sensitive feature\n",
    "label_columns = df.columns[-1]  # The last two columns as labels\n",
    "\n",
    "# Convert Pandas DataFrame to a TensorFlow dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    df[feature_columns].values.astype(np.float32),  # Features\n",
    "    df[sensitive_column].values.reshape(-1, 1).astype(np.int32),  # Sensitive Feature\n",
    "    df[label_columns].values.astype(np.int32)  # Ensure labels are integers\n",
    "))\n",
    "# Define batch size\n",
    "batch_size = 16  \n",
    "\n",
    "# Shuffle before batching\n",
    "buffer_size = len(df)  # Ideally, use the dataset size as the buffer\n",
    "shuffled_dataset = dataset.shuffle(buffer_size, seed=42).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sensitive_column' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msensitive_column\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sensitive_column' is not defined"
     ]
    }
   ],
   "source": [
    "sensitive_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processor(data, batch_size):\n",
    "\n",
    "    # Define features to standardize (Z-score)\n",
    "    standardize_features = [\"Red blood cells\", \"Hemoglobin\", \"Albumin\", \"Protein\"]\n",
    "\n",
    "    # Define features to normalize (Min-Max)\n",
    "    normalize_features = [\"ALT\", \"AST\", \"LDH\", \"Bilirubin\", \"Triglycerides\", \"Glucose\"]\n",
    "\n",
    "    # Categorical Columns\n",
    "    categorical_cols = ['Diabetes', 'Blood related diabetes', 'Vigorous-work']\n",
    "\n",
    "\n",
    "    # Data Processing Helper Functions\n",
    "    scaler = StandardScaler()\n",
    "    minmax_scaler = MinMaxScaler()\n",
    "    encoder = OneHotEncoder(sparse=False, drop=\"first\")  \n",
    "\n",
    "\n",
    "    # Binarize Binary Categorical Columns\n",
    "    data['Gender'] =  data['Gender'].mask(data['Gender'] == 'Male', 1).mask(data['Gender'] == 'Female', 0)\n",
    "    data['Coronary heart disease'] = data['Coronary heart disease'].mask(data['Coronary heart disease'] == 'Yes', 1).mask(data['Coronary heart disease'] == 'No', 0)\n",
    "\n",
    "    # Separate Data to Features and Labels\n",
    "    X = data.drop('Coronary heart disease', axis = 1)\n",
    "    y = data['Coronary heart disease']\n",
    "\n",
    "    \n",
    "    # Split Data into Train, Validatin, and Test Sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "    # One-hot Encoding and Standardization/Normalization of Data\n",
    "    X_train[standardize_features] = scaler.fit_transform(X_train[standardize_features])\n",
    "    X_val[standardize_features] = scaler.transform(X_val[standardize_features])\n",
    "    X_test[standardize_features] = scaler.transform(X_test[standardize_features])\n",
    "\n",
    "    X_train[normalize_features] = minmax_scaler.fit_transform(X_train[normalize_features])\n",
    "    X_val[normalize_features] = minmax_scaler.transform(X_val[normalize_features])\n",
    "    X_test[normalize_features] = minmax_scaler.transform(X_test[normalize_features])\n",
    "\n",
    "    # Convert to DataFrame with correct column names\n",
    "    X_train_encoded = encoder.fit_transform(X_train[categorical_cols])\n",
    "    X_val_encoded = encoder.transform(X_val[categorical_cols])\n",
    "    X_test_encoded = encoder.transform(X_test[categorical_cols])\n",
    "\n",
    "    train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoder.get_feature_names_out(categorical_cols), index=X_train.index)\n",
    "    val_encoded_df = pd.DataFrame(X_val_encoded, columns=encoder.get_feature_names_out(categorical_cols), index=X_val.index)\n",
    "    test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoder.get_feature_names_out(categorical_cols), index=X_test.index)\n",
    "\n",
    "    # Drop original categorical columns and concatenate the new one-hot encoded features\n",
    "    X_train = X_train.drop(columns=categorical_cols).reset_index(drop=True)\n",
    "    X_val = X_val.drop(columns=categorical_cols).reset_index(drop=True)\n",
    "    X_test = X_test.drop(columns=categorical_cols).reset_index(drop=True)\n",
    "\n",
    "    X_train = pd.concat([X_train, train_encoded_df], axis=1)\n",
    "    X_val = pd.concat([X_val, val_encoded_df], axis=1)\n",
    "    X_test = pd.concat([X_test, test_encoded_df], axis=1)\n",
    "\n",
    "    # Convert Pandas DataFrame to a TensorFlow dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    X_train.to_numpy().astype(np.float32),  # Features\n",
    "    X_train['Gender'].to_numpy().reshape(-1, 1).astype(np.int32),  # Sensitive Feature\n",
    "    y_train.to_numpy().values.astype(np.int32)  # Ensure labels are integers\n",
    "    ))\n",
    "\n",
    "    # Batch Training Set\n",
    "    buffer_size = len(X_train)  # Ideally, use the dataset size as the buffer\n",
    "    batched_dataset = dataset.shuffle(buffer_size, seed=42).batch(batch_size)\n",
    "\n",
    "    return batched_dataset, X_val, y_val, X_test, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features batch shape: (16, 40)\n",
      "Sensitive Feature batch shape: (16, 1)\n",
      "Labels batch shape: (16,)\n"
     ]
    }
   ],
   "source": [
    "for batch in shuffled_dataset.take(1):  \n",
    "    features, sensitive_features, labels = batch\n",
    "    print(\"Features batch shape:\", features.shape)  \n",
    "    print(\"Sensitive Feature batch shape:\", sensitive_features.shape)  \n",
    "    print(\"Labels batch shape:\", labels.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 unique labels: [1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 1]\n",
      "Batch 1 unique labels: [0 0 1 1 1 0 0 1 1 0 1 0 0 0 1 1]\n",
      "Batch 2 unique labels: [1 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1]\n",
      "Batch 3 unique labels: [0 1 1 0 1 1 1 1 0 0 0 1 1 0 0 0]\n",
      "Batch 4 unique labels: [0 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "for step, (x, z, labels) in enumerate(shuffled_dataset.take(5)):  # Check first 5 batches\n",
    "    unique_labels = np.unique(labels.numpy())\n",
    "    print(f\"Batch {step} unique labels: {labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialModel(keras.Model):\n",
    "    def __init__(self, input_dim, sensitive_attr,lambda_tradeoff=0.1, GBT_retrain = 5, epochs = 100):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize Attributes\n",
    "        self.lambda_tradeoff = lambda_tradeoff  # Trade-off parameter for adversarial penalty\n",
    "        self.sensitive_attr = sensitive_attr\n",
    "        self.epochs = epochs\n",
    "        self.GBT_retrain = GBT_retrain\n",
    "   \n",
    "        # Define the main neural network\n",
    "        self.dense1 = Dense(32, activation='relu', input_dim = input_dim)\n",
    "        self.dropout1 = Dropout(0.3)  # Added Dropout layer\n",
    "        self.dense2 = Dense(16, activation='relu')\n",
    "        self.output_layer = Dense(1, activation='sigmoid')  # Binary classification\n",
    "    \n",
    "        \n",
    "        # Metrics and optimizer for Main Model\n",
    "        self.loss_fn = keras.losses.BinaryCrossentropy(name=\"loss\")\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "        self.main_acc_metric = keras.metrics.BinaryAccuracy(name=\"accuracy\")\n",
    "\n",
    "        # Adversarial model (Gradient Boosted Trees)\n",
    "        self.adversarial_model = tfdf.keras.GradientBoostedTreesModel(task = tfdf.keras.Task.CLASSIFICATION)\n",
    "\n",
    "    def call(self, inputs, train = False):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dropout1(x, training = train)\n",
    "        x = self.dense2(x)\n",
    "        return self.output_layer(x)\n",
    "    \n",
    "    @tf.function\n",
    "    def fit(self, data):\n",
    "\n",
    "        # Number of Batches\n",
    "        num_batches = len(data)\n",
    "       \n",
    "        for epoch in range(self.epochs):\n",
    "            \n",
    "            # Epoch Progress Tracking\n",
    "            start_time = time.time()\n",
    "            print(f\"\\nEpoch {epoch + 1}/{self.epochs}\")\n",
    "            progbar = keras.utils.Progbar(target = num_batches)\n",
    "\n",
    "            # Track epoch loss\n",
    "            epoch_loss = 0\n",
    "\n",
    "            # Adversarial Model is Trained at every K Epochs\n",
    "            if epoch % self.GBT_retrain == 0:\n",
    "                y_preds = []\n",
    "                z_labels = []\n",
    "              \n",
    "\n",
    "                # Get Predictions For Most Recent Updated Main Model\n",
    "                for step, (X_batch_train, z_batch_train, _) in enumerate(data):\n",
    "                    y_preds.append(self(X_batch_train, train=False))  # No .numpy()\n",
    "                    z_labels.append(z_batch_train)  # No .numpy()\n",
    "\n",
    "                # ✅ Convert after exiting `@tf.function`\n",
    "                y_preds = tf.concat(y_preds, axis=0).numpy()  # Convert to NumPy AFTER\n",
    "                z_labels = tf.concat(z_labels, axis=0).numpy()\n",
    "\n",
    "                self.adversarial_model.fit(x=y_preds, y=z_labels)\n",
    "\n",
    "                # Compute Adversarial Model Loss\n",
    "                adversarial_model_loss  = self.adversarial_model.make_inspector().evaluation()[2]\n",
    "\n",
    "           \n",
    "            for step, (X_batch_train,_, y_batch_train) in enumerate(data):\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    # Forward pass\n",
    "                    y_pred = self(X_batch_train, train=True)\n",
    "\n",
    "                    # Compute Main Model Loss\n",
    "                    main_model_loss = self.loss_fn(y_batch_train, y_pred)\n",
    "\n",
    "                    # Compute Combined Loss\n",
    "                    combined_loss = main_model_loss + (main_model_loss / adversarial_model_loss + 1e-7) - (self.lambda_tradeoff * adversarial_model_loss)\n",
    "\n",
    "                # Compute gradients\n",
    "                gradients = tape.gradient(combined_loss, self.trainable_weights)\n",
    "\n",
    "                # Update weights\n",
    "                self.optimizer.apply_gradients(list(zip(gradients, self.trainable_weights)))\n",
    "\n",
    "          \n",
    "                 # Update training metric.\n",
    "                self.main_acc_metric.update_state(y_batch_train, y_pred)\n",
    "\n",
    "                # Track loss for epoch summary\n",
    "                epoch_loss += combined_loss.numpy()\n",
    "\n",
    "\n",
    "            # Update Progress Bar per batch\n",
    "            progbar.update(step + 1, values=[(\"loss\", float(combined_loss)), (\"accuracy\", float(self.main_acc_metric.result()))])\n",
    "             \n",
    "                    \n",
    "        # Final calculations per epoch\n",
    "        elapsed_time = time.time() - start_time\n",
    "        time_per_step = elapsed_time / num_batches * 1e6  # Convert to microseconds\n",
    "        final_accuracy = self.main_acc_metric.result().numpy()\n",
    "        final_loss = epoch_loss / num_batches\n",
    "\n",
    "        # Print final epoch stats\n",
    "        print(f\"\\n{num_batches}/{num_batches} - {int(elapsed_time)}s {int(time_per_step)}us/step - accuracy: {final_accuracy:.4f} - loss: {final_loss:.4f}\")\n",
    "\n",
    "        # Reset Accuracy for Next Epoch\n",
    "        self.main_acc_metric.reset_state()\n",
    "\n",
    "    def predict(self, X_input, threshold = None, raw_probabilities = None):\n",
    "\n",
    "    \n",
    "        if threshold is None:\n",
    "            threshold = 0.11\n",
    "\n",
    "        if raw_probabilities is None:\n",
    "            raw_probabilities = False\n",
    "\n",
    "        pred_proba = super().predict(X_input)\n",
    "\n",
    "        zpred_proba = self.adversarial_model.predict(pred_proba)\n",
    "\n",
    "        if raw_probabilities == True:\n",
    "\n",
    "            return pred_proba, zpred_proba\n",
    "        \n",
    "        else:\n",
    "             \n",
    "            binary_preds =  (pred_proba >= threshold).astype(int)\n",
    "            binary_zpreds = (zpred_proba >= threshold).astype(int)\n",
    "\n",
    "            return binary_preds, binary_zpreds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use /tmp/tmpvutgogyp as temporary training directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-02-03 18:53:54.218547: W external/ydf/yggdrasil_decision_forests/learner/gradient_boosted_trees/gradient_boosted_trees.cc:1840] \"goss_alpha\" set but \"sampling_method\" not equal to \"GOSS\".\n",
      "2025-02-03 18:53:54.218603: W external/ydf/yggdrasil_decision_forests/learner/gradient_boosted_trees/gradient_boosted_trees.cc:1850] \"goss_beta\" set but \"sampling_method\" not equal to \"GOSS\".\n",
      "2025-02-03 18:53:54.218625: W external/ydf/yggdrasil_decision_forests/learner/gradient_boosted_trees/gradient_boosted_trees.cc:1864] \"selective_gradient_boosting_ratio\" set but \"sampling_method\" not equal to \"SELGB\".\n"
     ]
    }
   ],
   "source": [
    "model = AdversarialModel(41,'Gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "InaccessibleTensorError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_372/871302263.py\", line 61, in fit  *\n        y_preds = tf.concat(y_preds, axis=0).numpy()  # Convert to NumPy AFTER\n    File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/tensorflow/core/function/capture/capture_container.py\", line 144, in capture_by_value\n        graph._validate_in_scope(tensor)  # pylint: disable=protected-access\n\n    InaccessibleTensorError: <tf.Tensor 'adversarial_model_3_1/dense_11_1/Sigmoid:0' shape=(None, 1) dtype=float32> is out of scope and cannot be used here. Use return values, explicit Python locals or TensorFlow collections to access it.\n    Please see https://www.tensorflow.org/guide/function#all_outputs_of_a_tffunction_must_be_return_values for more information.\n    \n    <tf.Tensor 'adversarial_model_3_1/dense_11_1/Sigmoid:0' shape=(None, 1) dtype=float32> was defined here:\n        File \"<frozen runpy>\", line 198, in _run_module_as_main\n        File \"<frozen runpy>\", line 88, in _run_code\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n        File \"/usr/lib/python3.11/asyncio/base_events.py\", line 604, in run_forever\n        File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1909, in _run_once\n        File \"/usr/lib/python3.11/asyncio/events.py\", line 80, in _run\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n        File \"/tmp/ipykernel_372/2030562094.py\", line 1, in <module>\n        File \"/tmp/ipykernel_372/871302263.py\", line 39, in fit\n        File \"/tmp/ipykernel_372/871302263.py\", line 50, in fit\n        File \"/tmp/ipykernel_372/871302263.py\", line 56, in fit\n        File \"/tmp/ipykernel_372/871302263.py\", line 57, in fit\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py\", line 908, in __call__\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/keras/src/ops/operation.py\", line 46, in __call__\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n        File \"/tmp/ipykernel_372/871302263.py\", line 31, in call\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py\", line 908, in __call__\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/keras/src/ops/operation.py\", line 46, in __call__\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/keras/src/layers/core/dense.py\", line 148, in call\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/keras/src/activations/activations.py\", line 498, in sigmoid\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/keras/src/ops/nn.py\", line 109, in sigmoid\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/nn.py\", line 24, in sigmoid\n    \n    The tensor <tf.Tensor 'adversarial_model_3_1/dense_11_1/Sigmoid:0' shape=(None, 1) dtype=float32> cannot be accessed from FuncGraph(name=fit, id=139658379690432), because it was defined in FuncGraph(name=Dataset_scan_scan_body, id=139658382048960), which is out of scope.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInaccessibleTensorError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshuffled_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DSC-180-Capstone/.venv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/DSC-180-Capstone/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py:52\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m     51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mInaccessibleTensorError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_372/871302263.py\", line 61, in fit  *\n        y_preds = tf.concat(y_preds, axis=0).numpy()  # Convert to NumPy AFTER\n    File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/tensorflow/core/function/capture/capture_container.py\", line 144, in capture_by_value\n        graph._validate_in_scope(tensor)  # pylint: disable=protected-access\n\n    InaccessibleTensorError: <tf.Tensor 'adversarial_model_3_1/dense_11_1/Sigmoid:0' shape=(None, 1) dtype=float32> is out of scope and cannot be used here. Use return values, explicit Python locals or TensorFlow collections to access it.\n    Please see https://www.tensorflow.org/guide/function#all_outputs_of_a_tffunction_must_be_return_values for more information.\n    \n    <tf.Tensor 'adversarial_model_3_1/dense_11_1/Sigmoid:0' shape=(None, 1) dtype=float32> was defined here:\n        File \"<frozen runpy>\", line 198, in _run_module_as_main\n        File \"<frozen runpy>\", line 88, in _run_code\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n        File \"/usr/lib/python3.11/asyncio/base_events.py\", line 604, in run_forever\n        File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1909, in _run_once\n        File \"/usr/lib/python3.11/asyncio/events.py\", line 80, in _run\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n        File \"/tmp/ipykernel_372/2030562094.py\", line 1, in <module>\n        File \"/tmp/ipykernel_372/871302263.py\", line 39, in fit\n        File \"/tmp/ipykernel_372/871302263.py\", line 50, in fit\n        File \"/tmp/ipykernel_372/871302263.py\", line 56, in fit\n        File \"/tmp/ipykernel_372/871302263.py\", line 57, in fit\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py\", line 908, in __call__\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/keras/src/ops/operation.py\", line 46, in __call__\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n        File \"/tmp/ipykernel_372/871302263.py\", line 31, in call\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py\", line 908, in __call__\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/keras/src/ops/operation.py\", line 46, in __call__\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/keras/src/layers/core/dense.py\", line 148, in call\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/keras/src/activations/activations.py\", line 498, in sigmoid\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/keras/src/ops/nn.py\", line 109, in sigmoid\n        File \"/home/dsilva/DSC-180-Capstone/.venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/nn.py\", line 24, in sigmoid\n    \n    The tensor <tf.Tensor 'adversarial_model_3_1/dense_11_1/Sigmoid:0' shape=(None, 1) dtype=float32> cannot be accessed from FuncGraph(name=fit, id=139658379690432), because it was defined in FuncGraph(name=Dataset_scan_scan_body, id=139658382048960), which is out of scope.\n"
     ]
    }
   ],
   "source": [
    "model.fit(shuffled_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Dummy set for Predict Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Number of rows and columns\n",
    "num_rows = 1000\n",
    "num_columns = 41\n",
    "\n",
    "# Generate random float data for 39 columns\n",
    "float_data = np.random.rand(num_rows, num_columns - 2)\n",
    "\n",
    "# Generate two binary columns\n",
    "binary_data = np.random.randint(0, 2, size=(num_rows, 2))\n",
    "\n",
    "# Combine into a single dataset\n",
    "data = np.hstack((float_data, binary_data))\n",
    "\n",
    "# Create column names\n",
    "column_names = [f\"feature_{i}\" for i in range(1, num_columns - 1)]\n",
    "column_names.append(\"Gender\")\n",
    "column_names.append(\"CHD\")\n",
    "\n",
    "# Create DataFrame\n",
    "df_predict = pd.DataFrame(data, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_31</th>\n",
       "      <th>feature_32</th>\n",
       "      <th>feature_33</th>\n",
       "      <th>feature_34</th>\n",
       "      <th>feature_35</th>\n",
       "      <th>feature_36</th>\n",
       "      <th>feature_37</th>\n",
       "      <th>feature_38</th>\n",
       "      <th>feature_39</th>\n",
       "      <th>Gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.762664</td>\n",
       "      <td>0.108972</td>\n",
       "      <td>0.784587</td>\n",
       "      <td>0.356771</td>\n",
       "      <td>0.992820</td>\n",
       "      <td>0.497898</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>0.936143</td>\n",
       "      <td>0.489978</td>\n",
       "      <td>0.564812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.403190</td>\n",
       "      <td>0.474390</td>\n",
       "      <td>0.497667</td>\n",
       "      <td>0.964042</td>\n",
       "      <td>0.672578</td>\n",
       "      <td>0.739626</td>\n",
       "      <td>0.633534</td>\n",
       "      <td>0.735760</td>\n",
       "      <td>0.127226</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.910774</td>\n",
       "      <td>0.566361</td>\n",
       "      <td>0.747105</td>\n",
       "      <td>0.025832</td>\n",
       "      <td>0.397878</td>\n",
       "      <td>0.476149</td>\n",
       "      <td>0.575739</td>\n",
       "      <td>0.326920</td>\n",
       "      <td>0.179086</td>\n",
       "      <td>0.149810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244063</td>\n",
       "      <td>0.824258</td>\n",
       "      <td>0.112698</td>\n",
       "      <td>0.260016</td>\n",
       "      <td>0.324769</td>\n",
       "      <td>0.226133</td>\n",
       "      <td>0.657886</td>\n",
       "      <td>0.571461</td>\n",
       "      <td>0.823201</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.995462</td>\n",
       "      <td>0.130282</td>\n",
       "      <td>0.655095</td>\n",
       "      <td>0.439705</td>\n",
       "      <td>0.732180</td>\n",
       "      <td>0.792697</td>\n",
       "      <td>0.527991</td>\n",
       "      <td>0.918847</td>\n",
       "      <td>0.477402</td>\n",
       "      <td>0.784071</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172986</td>\n",
       "      <td>0.657642</td>\n",
       "      <td>0.800218</td>\n",
       "      <td>0.668323</td>\n",
       "      <td>0.913286</td>\n",
       "      <td>0.725450</td>\n",
       "      <td>0.693683</td>\n",
       "      <td>0.245175</td>\n",
       "      <td>0.890300</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.298222</td>\n",
       "      <td>0.794529</td>\n",
       "      <td>0.353065</td>\n",
       "      <td>0.637051</td>\n",
       "      <td>0.130808</td>\n",
       "      <td>0.984601</td>\n",
       "      <td>0.920892</td>\n",
       "      <td>0.167425</td>\n",
       "      <td>0.025938</td>\n",
       "      <td>0.090521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608200</td>\n",
       "      <td>0.969789</td>\n",
       "      <td>0.790762</td>\n",
       "      <td>0.805609</td>\n",
       "      <td>0.484032</td>\n",
       "      <td>0.115285</td>\n",
       "      <td>0.612453</td>\n",
       "      <td>0.731313</td>\n",
       "      <td>0.953025</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.064734</td>\n",
       "      <td>0.612075</td>\n",
       "      <td>0.946083</td>\n",
       "      <td>0.724822</td>\n",
       "      <td>0.075990</td>\n",
       "      <td>0.373723</td>\n",
       "      <td>0.929611</td>\n",
       "      <td>0.171133</td>\n",
       "      <td>0.115018</td>\n",
       "      <td>0.420616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.686042</td>\n",
       "      <td>0.821443</td>\n",
       "      <td>0.215723</td>\n",
       "      <td>0.528370</td>\n",
       "      <td>0.043151</td>\n",
       "      <td>0.745641</td>\n",
       "      <td>0.889469</td>\n",
       "      <td>0.156075</td>\n",
       "      <td>0.644471</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0   0.762664   0.108972   0.784587   0.356771   0.992820   0.497898   \n",
       "1   0.910774   0.566361   0.747105   0.025832   0.397878   0.476149   \n",
       "2   0.995462   0.130282   0.655095   0.439705   0.732180   0.792697   \n",
       "3   0.298222   0.794529   0.353065   0.637051   0.130808   0.984601   \n",
       "4   0.064734   0.612075   0.946083   0.724822   0.075990   0.373723   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_31  feature_32  \\\n",
       "0   0.002740   0.936143   0.489978    0.564812  ...    0.403190    0.474390   \n",
       "1   0.575739   0.326920   0.179086    0.149810  ...    0.244063    0.824258   \n",
       "2   0.527991   0.918847   0.477402    0.784071  ...    0.172986    0.657642   \n",
       "3   0.920892   0.167425   0.025938    0.090521  ...    0.608200    0.969789   \n",
       "4   0.929611   0.171133   0.115018    0.420616  ...    0.686042    0.821443   \n",
       "\n",
       "   feature_33  feature_34  feature_35  feature_36  feature_37  feature_38  \\\n",
       "0    0.497667    0.964042    0.672578    0.739626    0.633534    0.735760   \n",
       "1    0.112698    0.260016    0.324769    0.226133    0.657886    0.571461   \n",
       "2    0.800218    0.668323    0.913286    0.725450    0.693683    0.245175   \n",
       "3    0.790762    0.805609    0.484032    0.115285    0.612453    0.731313   \n",
       "4    0.215723    0.528370    0.043151    0.745641    0.889469    0.156075   \n",
       "\n",
       "   feature_39  Gender  \n",
       "0    0.127226     0.0  \n",
       "1    0.823201     1.0  \n",
       "2    0.890300     1.0  \n",
       "3    0.953025     1.0  \n",
       "4    0.644471     1.0  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val = df_predict.drop([\"CHD\"], axis = 1)\n",
    "y_val = df.CHD\n",
    "x_val.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1.0\n",
       "1      0.0\n",
       "2      1.0\n",
       "3      1.0\n",
       "4      0.0\n",
       "      ... \n",
       "995    0.0\n",
       "996    0.0\n",
       "997    1.0\n",
       "998    0.0\n",
       "999    0.0\n",
       "Name: CHD, Length: 1000, dtype: float64"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "32/32 [==============================] - 0s 648us/step\n"
     ]
    }
   ],
   "source": [
    "y_preds, zpreds = model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),  # Adam optimizer\n",
    "    loss='binary_crossentropy', # Binary Crossentropy loss\n",
    "    metrics=['accuracy']  # Track accuracy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.9963  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9922277331352234, 0.0, 0.0]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.9963 \n",
      "[0.9922277331352234, 0.0, 0.0]\n",
      "Loss =  0.9922277331352234\n",
      "Accuracy =  0.0\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = model.evaluate(x_val, y_val)\n",
    "print(loss_and_metrics)\n",
    "print('Loss = ',loss_and_metrics[0])\n",
    "print('Accuracy = ',loss_and_metrics[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.5009999871253967>}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
