{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data obtained from:\n",
    "https://www.sciencedirect.com/science/article/abs/pii/S0957417420302323?casa_token=KAfym3lH_REAAAAA:VvMAXW-RJ7-ZNRJdoEhmO5zAdSXdmx4t6hCY1kifJYm_Q98DOlWMDqv6GL8ZFNDdrKfB-TRb08Q#sec0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_columns = [\n",
    "    \"Gender\", # Demographic: \"Demographic Variables & Sample Weights\" - RIAGENDR\n",
    "    \"Age\", # Demographic: \"Demographic Variables & Sample Weights\" - RIDAGEMN (in months)\n",
    "\n",
    "\n",
    "    \"Systolic\", # Examination: \"Blood Pressure\" - BPXSAR (BPXSY1, BPXSY2, BPXSY3, BPXSY4 ?)\n",
    "    \"Diastolic\", # Examination: \"Blood Pressure\" - BPXDAR (BPXDI1, BPXDI2, BPXDI3, BPXDI4 ?)\n",
    "\n",
    "    \"Weight\", # Examination: \"Body Measures\" - BMIWT\n",
    "    \"Body mass index\", # Examination: \"Body Measures\" - BMXBMI\n",
    "\n",
    "\n",
    "    \"White blood cells\", # Laboratory: \"LAB25\" - LBXWBCSI\n",
    "    \"Basophils\", # Laboratory: \"LAB25\" - LBDBANO (LBXBAPCT ?)\n",
    "    \"Red blood cells\", # Laboratory: \"LAB25\" - LBXRBCSI\n",
    "    \"Hemoglobin\", # Laboratory: \"LAB25\" - LBXHGB\n",
    "    \"Platelet count\", # Laboratory: \"LAB25\" - LBXPLTSI\n",
    "    \"Mean volume of platelets\", # Laboratory: \"LAB25\" - LBXMPSI\n",
    "    \"Red blood cell width\", # Laboratory: \"LAB25\" - LBXRDW\n",
    "\n",
    "    \"Creatinine\", # Laboratory: \"LAB18\" - LBDSCRSI # there is a bunch of different Creatinines\n",
    "    \"Glucose\", # Laboratory: \"LAB18\" - LBXSGL # there is a bunch of different Glucoses\n",
    "    \"Gamma-glutamyl transferase (GGT)\", # Laboratory: \"LAB18\" - LBXSGTSI\n",
    "    \"Iron\", # Laboratory: \"LAB18\" - LBDSIRSI # bunch of different Irons\n",
    "    \"Lactate dehydrogenase (LDH)\", # Laboratory: \"LAB18\" - LBXSLDSI\n",
    "    \"Phosphorus\", # Laboratory: \"LAB18\" - LBDSPHSI # bunch of different ones\n",
    "    \"Bilirubin\", # Laboratory: \"LAB18\" - LBDSTBSI # bunch of different ones\n",
    "    \"Protein\", # Laboratory: \"LAB18\" - LBDSTPSI # bunch of different ones\n",
    "    \"Uric acid\", # Laboratory: \"LAB18\" - LBDSUASI # bunch of different ones\n",
    "    \"Triglycerides\", # Laboratory: \"LAB18\" - LBDSTRSI # bunch of different ones\n",
    "    \"Albumin\", # Laboratory: \"LAB18\" - LBDSALSI \n",
    "    \"Alkaline phosphatase (ALP)\", # Laboratory: \"LAB18\" - LBXSAPSI\n",
    "    \"Aspartate aminotransferase (AST)\", # Laboratory: \"LAB18\" - LBXSASSI\n",
    "    \"Alanine aminotransferase (ALT)\", # Laboratory: \"LAB18\" - LBXSATSI\n",
    "\n",
    "    \"High-density lipoprotein (HDL)\", # Laboratory: \"Lab13\" - LBDHDLSI #???\n",
    "    \"Cholesterol\", # Laboratory: \"Lab13\" - LBDTCSI\n",
    "\n",
    "    \"Glycohemoglobin\", # Laboratory: \"LAB10\" - LBXGH\n",
    "\n",
    "\n",
    "    \"Vigorous-work\", # Questionnaire: \"Physical Activity\" - PAD200\n",
    "    \"Moderate-work\", # Questionnaire: \"Physical Activity\" - PAD120\n",
    "\n",
    "    \"Diabetes\", # Questionnaire: \"Diabetes\" - DIQ010\n",
    "\n",
    "    \"Blood related diabetes\", # Questionnaire: \"Medical Conditions\" - MCQ250A\n",
    "    \"Blood related stroke\", # Questionnaire: \"Medical Conditions\" - MCQ250F\n",
    "    \"Coronary heart Disease\" # Questionnaire: \"Medical Conditions\" - MCQ160C\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_specific_files(url, output_folder, filename):\n",
    "    \"\"\"\n",
    "    Downloads specific files hyperlinked on a webpage.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL of the webpage to scrape.\n",
    "        output_folder (str): The folder to save the downloaded files.\n",
    "        filenames (list): A list of specific file names to download.\n",
    "    \"\"\"\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    links = soup.find_all('a', href=True)\n",
    "\n",
    "    for link in links:\n",
    "        href = link['href']\n",
    "        \n",
    "        # Resolve relative URLs to absolute URLs\n",
    "        file_url = href if href.startswith('http') else requests.compat.urljoin(url, href)\n",
    "        \n",
    "        # Extract the file name from the URL\n",
    "        file_name = file_url.split('/')[-1]\n",
    "\n",
    "        if file_name == filename:\n",
    "            file_path = os.path.join(output_folder, file_name)\n",
    "            try:\n",
    "                file_response = requests.get(file_url, stream=True)\n",
    "                file_response.raise_for_status() \n",
    "                \n",
    "                # Save the file\n",
    "                with open(file_path, 'wb') as file:\n",
    "                    for chunk in file_response.iter_content(chunk_size=8192):\n",
    "                        file.write(chunk)\n",
    "\n",
    "                return\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {file_url}: {e}\")\n",
    "                raise e\n",
    "    raise Exception\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Files that contain all of our attributes:\n",
    "- Demographic: Demographic Variables & Sample Weights - DEMO.xpt\n",
    "- Examination: Blood Pressure - BPX.xpt\n",
    "- Examination: Body Measures - BMX.xpt\n",
    "- Laboratory: Lab25 - LAB25.xpt\n",
    "- Laboratory: l18_2_00 - LAB18.xpt\n",
    "- Laboratory: LAB18 - LAB18.xpt\n",
    "- Laboratory: Lab13 - LAB13.xpt\n",
    "- Laboratory: l10_2_00 - LAB10.xpt\n",
    "- Questionnaire: Physical Activity - PAQ.xpt\n",
    "- Questionnaire: Diabetes - DIQ.xpt\n",
    "- Questionnaire: Medical Conditions - MCQ.xpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1999-2000\n",
    "file_dictionary = {\n",
    "    'Demographics': [\"DEMO.xpt\"],\n",
    "    \"Examination\": [\"BPX.xpt\", \"BMX.xpt\"],\n",
    "    \"Laboratory\": [\"LAB25.xpt\", \"LAB18.xpt\", \"LAB13.xpt\", \"LAB18.xpt\", \"LAB10.xpt\"],\n",
    "    \"Questionnaire\": [\"PAQ.xpt\", \"DIQ.xpt\", \"MCQ.xpt\"]\n",
    "}\n",
    "\n",
    "# 2001-2002: B\n",
    "# 2003-2004: C\n",
    "file_dictionary = {\n",
    "    'Demographics': [\"DEMO_E.xpt\"],\n",
    "    \"Examination\": [\"BPX_E.xpt\", \"BMX_E.xpt\"],\n",
    "    \"Laboratory\": [\"L25_D.xpt\", \"L10_D.xpt\", \"L13_D.xpt\", \"L40_D.xpt\"],\n",
    "    \"Questionnaire\": [\"PAQ_E.xpt\", \"DIQ_E.xpt\", \"MCQ_E.xpt\"]\n",
    "}\n",
    "\n",
    "# 2005-2006: D\n",
    "# 2007-2008: E\n",
    "# 2009-2010: F\n",
    "# 2011-2012: G\n",
    "# 2013-2014: H\n",
    "# 2015-2016: I\n",
    "file_dictionary = {\n",
    "    'Demographics': [\"DEMO_I.xpt\"],\n",
    "    \"Examination\": [\"BPX_I.xpt\", \"BMX_I.xpt\"],\n",
    "    \"Laboratory\": [\"BIOPRO_I.xpt\", \"CBC_I.xpt\", \"GHB_I.xpt\", \"HDL_I.xpt\", 'TCHOL_I.xpt'],\n",
    "    \"Questionnaire\": [\"PAQ_I.xpt\", \"DIQ_I.xpt\", \"MCQ_I.xpt\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading directory: \"Demographics\"\n",
      " - Reading in file: \"DEMO_I.xpt\" [SUCCESS]\n",
      "Reading directory: \"Examination\"\n",
      " - Reading in file: \"BPX_I.xpt\" [SUCCESS]\n",
      " - Reading in file: \"BMX_I.xpt\" [SUCCESS]\n",
      "Reading directory: \"Laboratory\"\n",
      " - Reading in file: \"BIOPRO_I.xpt\" [SUCCESS]\n",
      " - Reading in file: \"CBC_I.xpt\" [SUCCESS]\n",
      " - Reading in file: \"GHB_I.xpt\" [SUCCESS]\n",
      " - Reading in file: \"HDL_I.xpt\" [SUCCESS]\n",
      " - Reading in file: \"TCHOL_I.xpt\" [SUCCESS]\n",
      "Reading directory: \"Questionnaire\"\n",
      " - Reading in file: \"PAQ_I.xpt\" [SUCCESS]\n",
      " - Reading in file: \"DIQ_I.xpt\" [SUCCESS]\n",
      " - Reading in file: \"MCQ_I.xpt\" [SUCCESS]\n"
     ]
    }
   ],
   "source": [
    "webpage_url = 'https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component={}&CycleBeginYear=2015'\n",
    "output_directory = 'data_test'\n",
    "\n",
    "for directory, files in file_dictionary.items():\n",
    "    print(f'Reading directory: \"{directory}\"')\n",
    "    formatted_url = webpage_url.format(directory)\n",
    "    for file in files:\n",
    "        print(f' - Reading in file: \"{file}\"', end=\"\")\n",
    "        try:\n",
    "            download_specific_files(formatted_url, output_directory, file)\n",
    "            print(' [SUCCESS]')\n",
    "        except Exception as e:\n",
    "            print(' [FAIL]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Check Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def check_attributes(directory, year_key):\n",
    "    file_path = \"data_attributes.json\"\n",
    "\n",
    "    # Read the dictionary from the JSON file\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data_attribute_dict = json.load(file)\n",
    "        \n",
    "    attribute_tracker = {attribute:0 for attribute in data_attribute_dict[year_key]}\n",
    "\n",
    "    for file_name in os.listdir(directory):\n",
    "        try:\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "\n",
    "            file_df = pd.read_sas(file_path, format='xport')\n",
    "            \n",
    "            for column in file_df.columns:\n",
    "                if column in attribute_tracker:\n",
    "                    attribute_tracker[column] += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print('FAILED\\n')\n",
    "\n",
    "    for attribute, count in attribute_tracker.items():\n",
    "        if count == 0:\n",
    "            print(f'FAILED: {attribute}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"data_attributes.json\"\n",
    "\n",
    "# Read the dictionary from the JSON file\n",
    "with open(file_path, \"r\") as file:\n",
    "    data_attribute_dict = json.load(file)\n",
    "\n",
    "len(data_attribute_dict['2001-2002'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILED: MCQ250F\n",
      "FAILED: MCQ250F\n",
      "FAILED: MCQ250F\n",
      "FAILED: MCQ250F\n",
      "FAILED: MCQ250F\n",
      "FAILED: MCQ250F\n"
     ]
    }
   ],
   "source": [
    "check_attributes('data/1999_2000/', '1999-2000')\n",
    "check_attributes('data/2001_2002/', '2001-2002')\n",
    "check_attributes('data/2003_2004/', '2003-2004')\n",
    "check_attributes('data/2005_2006/', '2005-2006')\n",
    "check_attributes('data/2007_2008/', '2007-2008')\n",
    "check_attributes('data/2009_2010/', '2009-2010')\n",
    "check_attributes('data/2011_2012/', '2011-2012')\n",
    "check_attributes('data/2013_2014/', '2013-2014')\n",
    "check_attributes('data/2015_2016/', '2015-2016')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now select only the features we are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_selected_attributes_dictionary = {\n",
    "    \"DEMO.xpt\": ['RIAGENDR', 'RIDAGEMN'],\n",
    "    \"BPX.xpt\": ['BPXSAR', 'BPXDAR'],\n",
    "    \"BMX.xpt\": ['BMIWT', 'BMXBMI'],\n",
    "    \"LAB25.xpt\": ['LBXWBCSI', 'LBDBANO', 'LBXRBCSI', 'LBXHGB', 'LBXPLTSI', 'LBXMPSI', 'LBXRDW'],\n",
    "    \"LAB18.xpt\": ['LBDSCRSI', 'LBXSGL', 'LBXSGTSI', 'LBDSIRSI', 'LBXSLDSI', 'LBDSPHSI', 'LBDSTBSI', \\\n",
    "                  'LBDSTPSI', 'LBDSUASI', 'LBDSTRSI', 'LBDSALSI', 'LBXSAPSI',  'LBXSASSI', 'LBXSATSI'],\n",
    "    \"LAB13.xpt\": ['LBDHDLSI', 'LBDTCSI'],\n",
    "    \"LAB10.xpt\": ['LBXGH'],\n",
    "    \"PAQ.xpt\": ['PAD200', 'PAD120'], \n",
    "    \"DIQ.xpt\": ['DIQ010'], \n",
    "    \"MCQ.xpt\": ['MCQ250A', 'MCQ250F', 'MCQ160C']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attributes(directory):\n",
    "    for file_name in os.listdir(directory):\n",
    "        try:\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            selected_attributes = file_selected_attributes_dictionary[file_name]\n",
    "\n",
    "            file_df = pd.read_sas(file_path, format='xport')\n",
    "            print(selected_attributes)\n",
    "            print(file_path)\n",
    "            file_df[selected_attributes]\n",
    "            print('SUCCESS\\n')\n",
    "        except Exception as e:\n",
    "            print('FAILED\\n')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BMIWT', 'BMXBMI']\n",
      "data\\BMX.xpt\n",
      "SUCCESS\n",
      "\n",
      "['BPXSAR', 'BPXDAR']\n",
      "data\\BPX.xpt\n",
      "SUCCESS\n",
      "\n",
      "['RIAGENDR', 'RIDAGEMN']\n",
      "data\\DEMO.xpt\n",
      "SUCCESS\n",
      "\n",
      "['DIQ010']\n",
      "data\\DIQ.xpt\n",
      "SUCCESS\n",
      "\n",
      "['LBXGH']\n",
      "data\\LAB10.xpt\n",
      "SUCCESS\n",
      "\n",
      "['LBDHDLSI', 'LBDTCSI']\n",
      "data\\LAB13.xpt\n",
      "SUCCESS\n",
      "\n",
      "['LBXSCR', 'LBXSGL', 'LBXSGTSI', 'LBDSIRSI', 'LBXSLDSI', 'LBDSPHSI', 'LBDSTBSI', 'LBDSTPSI', 'LBDSUASI', 'LBDSTRSI', 'LBDSALSI', 'LBXSAPSI', 'LBXSASSI', 'LBXSATSI']\n",
      "data\\LAB18.xpt\n",
      "SUCCESS\n",
      "\n",
      "['LBXWBCSI', 'LBDBANO', 'LBXRBCSI', 'LBXHGB', 'LBXPLTSI', 'LBXMPSI', 'LBXRDW']\n",
      "data\\LAB25.xpt\n",
      "SUCCESS\n",
      "\n",
      "['MCQ250A', 'MCQ250F', 'MCQ160C']\n",
      "data\\MCQ.xpt\n",
      "SUCCESS\n",
      "\n",
      "['PAD200', 'PAD120']\n",
      "data\\PAQ.xpt\n",
      "SUCCESS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extract_attributes('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://drive.google.com/drive/folders/1Rf0Q4kPQblsORsRy1sHXRgjh-Ljlv3X2...\n",
      "Failed to download https://drive.google.com/drive/folders/1Rf0Q4kPQblsORsRy1sHXRgjh-Ljlv3X2: [Errno 22] Invalid argument: 'downloaded_files\\\\https://drive.google.com/drive/folders/1Rf0Q4kPQblsORsRy1sHXRgjh-Ljlv3X2.xpt'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Base directory for saving files locally\n",
    "base_directory = \"downloaded_files\"\n",
    "\n",
    "# List of Google Drive folder URLs (convert folder to file download URLs manually if needed)\n",
    "file_urls = [\n",
    "    \"https://drive.google.com/drive/folders/1Rf0Q4kPQblsORsRy1sHXRgjh-Ljlv3X2\",\n",
    "]\n",
    "\n",
    "# Download and save .xpt files\n",
    "for file_url in file_urls:\n",
    "    file_id = file_url.split(\"id=\")[-1]  # Extract the file ID\n",
    "    file_name = f\"{file_id}.xpt\"  # Use a default name if filenames aren't known\n",
    "    file_path = os.path.join(base_directory, file_name)\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(base_directory, exist_ok=True)\n",
    "\n",
    "    # Download the file\n",
    "    try:\n",
    "        print(f\"Downloading {file_url}...\")\n",
    "        response = requests.get(file_url, stream=True)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "        # Save the file\n",
    "        with open(file_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "\n",
    "        print(f\"Saved to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {file_url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
